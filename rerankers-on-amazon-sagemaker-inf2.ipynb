{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b7733e-f3ef-4fde-87d2-c4e9b14e6d54",
   "metadata": {},
   "source": [
    "# Deploy and benchmark reranker models on Inferentia2 using Amazon SageMaker\n",
    "\n",
    "In information retrieval and natural language processing applications, rerankers have emerged as powerful tools to enhance the accuracy and relevance of search results. Rerankers are specialized techniques or machine learning models designed to optimize the ordering of a set of retrieved items to improve the overall quality of information retrieval systems.\n",
    "\n",
    "The objective of this notebook is to demonstrate how you can deploy and scale reranker models using on Inferentia2 using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b234a66-a1e9-40ff-ab05-8e8da8a82ea1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc439ca3-1bec-433d-ba81-4418fcd2c0e7",
   "metadata": {},
   "source": [
    "Upgrade the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117db275-70b7-46b6-816f-15fac67190f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -U transformers sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661746f-2823-4a76-882d-1001805b0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "! python -m pip install --upgrade-strategy eager optimum[neuronx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a7bbb-7dd2-4a0c-9e49-c8d7bc2be342",
   "metadata": {},
   "source": [
    "Instantiate the necessary session paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db36da9-413f-415d-a3bd-6ff4337d022d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.djl_inference.model import DJLModel\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    sagemaker_runtime_client=boto3.client(\n",
    "        \"sagemaker-runtime\",\n",
    "        config=Config(\n",
    "            connect_timeout=10, retries={\"mode\": \"standard\", \"total_max_attempts\": 20}\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ea565-34ac-4674-bb94-72fca0a51f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download and compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7678fb-7308-46ae-a6de-d14a8954c51f",
   "metadata": {},
   "source": [
    "In this section, we will download and compile the model using a SageMaker training job. We will implement the following steps:\n",
    "1. Write the compilation script\n",
    "2. Create a Hugging Face SageMaker Estimator to instatiate the compilation job\n",
    "3. Run the compilation job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471a591-81b0-4c66-b28e-ae7a79777b56",
   "metadata": {},
   "source": [
    "In the compilation script, we use Hugging Face Optimum Neuron Library which is the interface between the ðŸ¤— Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4145172a-9564-459d-90c0-e67797bafd24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0fd8e-3270-47da-846b-b75658620cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/compile_reranker.py\n",
    "import os\n",
    "import tarfile\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from optimum.neuron import NeuronModelForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = 'BAAI/bge-reranker-v2-m3'\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Create the input preprocessor and model\n",
    "    model = NeuronModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        export=True,\n",
    "        batch_size=2,\n",
    "        dynamic_batch_size=True,\n",
    "        sequence_length=2048,\n",
    "        auto_cast_type=\"fp16\"\n",
    "    )\n",
    "    \n",
    "    # Save the TorchScript for inference deployment\n",
    "    model.save_pretrained(\"/opt/ml/model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa152d5-eba6-43e8-ae14-953b35df5639",
   "metadata": {},
   "source": [
    "Now, let's instantiate a Hugging Face SageMaker estimator referencing the compilation script, and the Deep Learning Container (DLC) to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b8b09-3975-4dbb-8783-36b5a48e3718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "instance_type = \"ml.trn1.2xlarge\"\n",
    "model_name = \"BAAI/bge-reranker-v2-m3\"\n",
    "save_directory = \"bge-reranker-v2-m3\"\n",
    "\n",
    "s3_model_path = f\"s3://{bucket}/compiled_models/{model_name}\"\n",
    "\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    entry_point=\"compile_reranker.py\",\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=s3_model_path,\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-training-neuronx:1.13.1-transformers4.36.2-neuronx-py310-sdk2.18.0-ubuntu20.04\",\n",
    "    volume_size=128,\n",
    "    py_version=\"py310\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2357a88-a0e3-44b9-ae80-5739a69c2c1e",
   "metadata": {},
   "source": [
    "Run the compilation job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b31230-c258-4b17-a26a-0fa3e031e6af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312526b4-0e69-4e3c-8b98-5cd32c7e4f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "s3_model_uri = S3Downloader.download(\n",
    "    s3_uri=f\"{s3_model_path}/{estimator._current_job_name}/output/model/\",\n",
    "    local_path=save_directory,\n",
    ")\n",
    "print(f\"model artifcats downloaded to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3802719-6569-4e5b-bed1-3fc4a9fd62c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare the inference script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d594c-d1b0-4603-a2e6-2b04a06996a6",
   "metadata": {},
   "source": [
    "In this section, we will provide an inference script to perform customize the preprocessing and the postprocessing of the reranking requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f119a56-b1e5-4d3a-9ce0-ee085c1b6ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir {save_directory}/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d93d49-409e-4f85-96c2-2170b95277d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {save_directory}/code/inference.py\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "from optimum.neuron import NeuronModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch_neuronx\n",
    " \n",
    "def model_fn(model_dir, temp=None):\n",
    "    # load local converted model and  tokenizer\n",
    "    model = NeuronModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    return model, tokenizer\n",
    " \n",
    " \n",
    "def predict_fn(data, pipeline):\n",
    "    model, tokenizer = pipeline\n",
    " \n",
    "    # extract body\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    print(inputs)\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(inputs,return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.neuron[\"static_sequence_length\"])\n",
    " \n",
    "    # Compute embeddings\n",
    "    with torch.no_grad():\n",
    "        scores = model(**encoded_input, return_dict=True).logits.view(-1, ).float()\n",
    "        scores = torch.sigmoid(scores).tolist()\n",
    " \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f1aff-fdd9-4384-be27-629b8cb188e8",
   "metadata": {},
   "source": [
    "## Upload the model artefacts to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec12baa-821f-4a7d-a369-151f0e5b3724",
   "metadata": {},
   "source": [
    "First, compress the model artefacts and inference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214844a-716f-4584-b231-60933d5484db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd {save_directory}\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd804b34-66a3-49dc-a2af-4b98003ce732",
   "metadata": {},
   "source": [
    "Upload to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca3334-3342-4928-8bf2-7fd0d4a17027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{bucket}/inference_artefacts/{model_name}\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(\n",
    "    local_path=f\"{save_directory}/model.tar.gz\", desired_s3_uri=s3_model_path\n",
    ")\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d11b9-f78c-4193-8a8f-2d1fdf0e0822",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Hugging Face SageMaker Model Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39e539-f25d-4af2-9692-651203e53017",
   "metadata": {},
   "source": [
    "In this section, we instantiate a Hugging Face SageMaker Model Object and reference the model artifcats in Amazon S3. We also choose the appropriate Hugging Facec Deep Learning Container image for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24207274-bcb7-456c-a5b6-79ebf3479c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "model_reranker = HuggingFaceModel(\n",
    "    model_data=s3_model_uri,  # path to your model.tar.gz on s3\n",
    "    role=role,  # iam role with permissions to create an Endpoint\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference-neuronx:2.1.2-transformers4.36.2-neuronx-py310-sdk2.18.0-ubuntu20.04\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00865c9-711c-4fdf-a34c-1989e9aef7a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the Model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662baf5a-f2f7-4578-8ead-acb248f94bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_reranker._is_compiled_model = True\n",
    "\n",
    "model_reranker_predictor = model_reranker.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.inf2.xlarge\",\n",
    "    volume_size=100,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1aa68d-425b-44dd-b462-4deba1e683c1",
   "metadata": {},
   "source": [
    "Once the model is deployed, test the model invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacfe00-6080-410f-a5c4-bf31f88692f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\"what is panda?\", \"hi\"],\n",
    "        [\n",
    "            \"what is panda?\",\n",
    "            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "        ],\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c1f57-4f35-4894-a530-5bff37167c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bge_rerank.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db3f0d-4b86-49f8-aeea-5f270d15df4b",
   "metadata": {},
   "source": [
    "## Benchmark the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619a4df-27af-4f94-93df-39abfd48db2f",
   "metadata": {},
   "source": [
    "Create a benchmark scrip that sends concurrent requests, stores and plots the latencies and throughputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8365d-5cc5-4f9e-873a-bd435cf76dd6",
   "metadata": {},
   "source": [
    "Benchmark the endpoint and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61353888-25e1-4549-be45-55b9500ac062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming predictor is already defined and initialized\n",
    "# and predictor.predict(data=payload) is the method to be benchmarked\n",
    "\n",
    "\n",
    "def benchmark_predictor(predictor, payload, steps, iterations=5):\n",
    "    \"\"\"\n",
    "    Benchmarks a predictor's performance by measuring latency and throughput\n",
    "    under varying levels of concurrent requests.\n",
    "\n",
    "    Args:\n",
    "        predictor (object): The predictor object with a `predict` method.\n",
    "        payload (any): The input data to be sent to the predictor.\n",
    "        steps (list): A list of different numbers of concurrent requests to test.\n",
    "        iterations (int, optional): The number of iterations for each concurrency level. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Three lists containing the request counts, latencies, and throughputs.\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "    request_counts = []\n",
    "\n",
    "    def send_request():\n",
    "        \"\"\"Sends a single request to the predictor and measures its latency.\"\"\"\n",
    "        start_time = time.time()\n",
    "        resp = predictor.predict(data=payload)\n",
    "        latency = time.time() - start_time\n",
    "        return latency\n",
    "\n",
    "    for num_requests in steps:\n",
    "        iter_latencies = []\n",
    "        iter_throughputs = []\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Use ThreadPoolExecutor to send concurrent requests\n",
    "            with concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=num_requests\n",
    "            ) as executor:\n",
    "                futures = [executor.submit(send_request) for _ in range(num_requests)]\n",
    "                latencies_batch = [\n",
    "                    future.result()\n",
    "                    for future in concurrent.futures.as_completed(futures)\n",
    "                ]\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "\n",
    "            # Calculate average latency for this iteration\n",
    "            latency = np.mean(latencies_batch)\n",
    "            # Calculate throughput for this iteration\n",
    "            throughput = num_requests / total_time\n",
    "\n",
    "            iter_latencies.append(latency)\n",
    "            iter_throughputs.append(throughput)\n",
    "\n",
    "        # Calculate average latency and throughput over all iterations\n",
    "        avg_latency = np.mean(iter_latencies)\n",
    "        avg_throughput = np.mean(iter_throughputs)\n",
    "\n",
    "        latencies.append(avg_latency)\n",
    "        throughputs.append(avg_throughput)\n",
    "        request_counts.append(num_requests)\n",
    "\n",
    "        # Print results for the current number of requests\n",
    "        print(\n",
    "            f\"Requests: {num_requests}, Average Latency: {avg_latency:.4f}s, Average Throughput: {avg_throughput:.2f} req/s\"\n",
    "        )\n",
    "\n",
    "    return request_counts, latencies, throughputs\n",
    "\n",
    "\n",
    "def plot_metrics(request_counts, latencies, throughputs):\n",
    "    \"\"\"\n",
    "    Plots the benchmarking results, showing the average latency and throughput\n",
    "    as a function of the number of concurrent requests.\n",
    "\n",
    "    Args:\n",
    "        request_counts (list): The list of different numbers of concurrent requests tested.\n",
    "        latencies (list): The list of average latencies corresponding to the request counts.\n",
    "        throughputs (list): The list of average throughputs corresponding to the request counts.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = \"tab:blue\"\n",
    "    ax1.set_xlabel(\"Number of Concurrent Requests\")\n",
    "    ax1.set_ylabel(\"Average Latency (s)\", color=color)\n",
    "    ax1.plot(request_counts, latencies, color=color)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = \"tab:green\"\n",
    "    ax2.set_ylabel(\n",
    "        \"Throughput (requests/s)\", color=color\n",
    "    )  # we already handled the x-label with ax1\n",
    "    ax2.plot(request_counts, throughputs, color=color)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(\"Latency and Throughput Benchmarking\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_latency_vs_throughput(latencies, throughputs, request_counts):\n",
    "    \"\"\"\n",
    "    Plots latency against throughput, with annotations for the number of concurrent requests.\n",
    "\n",
    "    Args:\n",
    "        latencies (list): The list of average latencies.\n",
    "        throughputs (list): The list of average throughputs.\n",
    "        request_counts (list): The list of different numbers of concurrent requests tested.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(throughputs, latencies, \"o-\")\n",
    "    plt.xlabel(\"Throughput (requests/s)\")\n",
    "    plt.ylabel(\"Average Latency (s)\")\n",
    "    plt.title(\"Latency vs Throughput\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Label each point with the request count\n",
    "    for i, request_count in enumerate(request_counts):\n",
    "        plt.annotate(\n",
    "            request_count,\n",
    "            (throughputs[i], latencies[i]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 10),\n",
    "            ha=\"center\",\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3d5f2-698c-4c25-b8d0-df67add71e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_benchmark = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"client_batch_size\",\n",
    "        \"concurrent_request_counts\",\n",
    "        \"latencies\",\n",
    "        \"throughputs\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "min_requests = 0\n",
    "max_requests = 5\n",
    "step_size = 1\n",
    "iterations = 5\n",
    "client_batch_size = 8\n",
    "\n",
    "steps = list(map(lambda x: 2**x, range(min_requests, max_requests, step_size)))\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        \"what is panda?\",\n",
    "        \"A panda is a type of bear that is known for its distinctive black and white coloring.\",\n",
    "    ]]\n",
    "    * client_batch_size\n",
    "}\n",
    "\n",
    "\n",
    "request_counts, latencies, throughputs = benchmark_predictor(\n",
    "    model_bge_rerank, payload, steps, iterations\n",
    ")\n",
    "plot_metrics(request_counts, latencies, throughputs)\n",
    "plot_latency_vs_throughput(latencies, throughputs, request_counts)\n",
    "\n",
    "new_data = {\n",
    "    \"client_batch_size\": [client_batch_size] * len(request_counts),\n",
    "    \"concurrent_request_counts\": request_counts,\n",
    "    \"latencies\": latencies,\n",
    "    \"throughputs\": throughputs,\n",
    "}\n",
    "\n",
    "df_benchmark = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"client_batch_size\",\n",
    "        \"concurrent_request_counts\",\n",
    "        \"latencies\",\n",
    "        \"throughputs\",\n",
    "    ]\n",
    ")\n",
    "df_benchmark = df_benchmark.append(pd.DataFrame(new_data), ignore_index=True)\n",
    "df_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e53c1-2a48-4a33-af64-6d42918e2653",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8e990-bcb2-49f0-935e-0768829a1b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bge_rerank_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
