{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b7733e-f3ef-4fde-87d2-c4e9b14e6d54",
   "metadata": {},
   "source": [
    "# Deploy and benchmark reranker models on Amazon SageMaker\n",
    "\n",
    "In information retrieval and natural language processing applications, rerankers have emerged as powerful tools to enhance the accuracy and relevance of search results. Rerankers are specialized techniques or machine learning models designed to optimize the ordering of a set of retrieved items to improve the overall quality of information retrieval systems.\n",
    "\n",
    "The objective of this notebook is to demonstrate how you can deploy and scale reranker models using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b234a66-a1e9-40ff-ab05-8e8da8a82ea1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc439ca3-1bec-433d-ba81-4418fcd2c0e7",
   "metadata": {},
   "source": [
    "Upgrade the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117db275-70b7-46b6-816f-15fac67190f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -U transformers hf_transfer sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a7bbb-7dd2-4a0c-9e49-c8d7bc2be342",
   "metadata": {},
   "source": [
    "Instantiate the necessary session paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db36da9-413f-415d-a3bd-6ff4337d022d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "from sagemaker.parameter import CategoricalParameter \n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import json\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    sagemaker_runtime_client=boto3.client(\n",
    "        \"sagemaker-runtime\",\n",
    "        config=Config(connect_timeout=10, retries={\"mode\": \"standard\", \"total_max_attempts\": 20}),\n",
    "    )\n",
    ")\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d11b9-f78c-4193-8a8f-2d1fdf0e0822",
   "metadata": {},
   "source": [
    "## Create Model Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f3b65-296c-4dcb-a24b-fd3d057177db",
   "metadata": {},
   "source": [
    "In this section, we create the SageMaker Model object. You can test the 3 options below:\n",
    "1. Create a SageMaker Jumpstart Model Object\n",
    "2. Create a HuggingFase SageMaker Model Object from a model downloaded from HuggingFace Hub\n",
    "3. Create a HuggingFase SageMaker Model Object from a model downloaded from Amazon S3\n",
    "4. Create a DJL Model Object that uses SageMaker Large Model Inference (LMI) container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9518168-b5cc-43c1-9691-9f38f6c7ee12",
   "metadata": {},
   "source": [
    "### Option 1: Create a SageMaker Jumpstart Model Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc1a0c-4d09-4d2e-b6c8-b0ff4587d8b9",
   "metadata": {},
   "source": [
    "Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey where you can can compare, and select foundation models (FMs) based on your use case like article summarization and image generation. You can fine-tune or deploy FMs in SageMaker Jumpstart via SageMaker Studio or SDK. You can find the full list of foundation models [here](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68117b3-70da-4c4d-a9bd-7b986f213986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# create Jumpstart Model object\n",
    "model_bge_rerank = JumpStartModel(\n",
    "    model_id='cohere-rerank-multilingual-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613ab31-09bd-47b3-8574-5d643896e013",
   "metadata": {},
   "source": [
    "### Option 2: Create a HuggingFace SageMaker Model Object from a model downloaded from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8cf13-51e5-4254-8038-3ba41d5a2f08",
   "metadata": {},
   "source": [
    "To deploy a model directly from the ðŸ¤— Hub to SageMaker, define an environment variable when you create a HuggingFaceModel:\n",
    "\n",
    "* HF_MODEL_ID defines the model ID which is automatically loaded from [huggingface.co/models](huggingface.co/models) when you create a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d5671-b493-41e7-a7fe-0eb33188e399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_hf = {\n",
    "    'HF_MODEL_ID':'BAAI/bge-reranker-v2-m3',\n",
    "    'DTYPE':'float16'\n",
    "}\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(config_hf[\"HF_MODEL_ID\"].replace(\"/\",\"-\"))\n",
    "\n",
    "model_bge_rerank = HuggingFaceModel(\n",
    "    image_uri=get_huggingface_llm_image_uri(\"huggingface-tei\"),\n",
    "    env=config_hf,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf2208-4c4b-4ac0-ba0c-2643b722b0b8",
   "metadata": {},
   "source": [
    "### Option 3: Create a HuggingFace SageMaker Model Object from a model downloaded from Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39e539-f25d-4af2-9692-651203e53017",
   "metadata": {},
   "source": [
    "If you dont want Amazon SageMaker to download the model artifcats from the HuggingFace hub when starting the inference endpoint, you can store the model on Amazon S3.\n",
    "\n",
    "First, download the model artifacts locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12812ce8-e33a-460e-b57c-85edc3228b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    " \n",
    "# set HF_HUB_ENABLE_HF_TRANSFER env var to enable hf-transfer for faster downloads\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "from huggingface_hub import snapshot_download\n",
    " \n",
    "HF_MODEL_ID = \"BAAI/bge-reranker-v2-m3\"\n",
    "# create model dir\n",
    "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
    "model_tar_dir.mkdir(exist_ok=True)\n",
    " \n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(\n",
    "    HF_MODEL_ID,\n",
    "    local_dir=str(model_tar_dir), # download to model dir\n",
    "    revision=\"main\", # use a specific revision, e.g. refs/pr/21\n",
    "    ignore_patterns=[\"*.msgpack*\", \"*.h5*\", \"*.bin*\", \"assets*\"], # to load safetensor weights\n",
    ")\n",
    " \n",
    "# check if safetensor weights are downloaded and available\n",
    "assert len(list(model_tar_dir.glob(\"*.safetensors\"))) > 0, \"Model download failed\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e158f7-09c7-4a39-b1cc-618aa522e30a",
   "metadata": {},
   "source": [
    "Compress the model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430a322-af51-4a7d-847a-0879a80bca64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parent_dir=os.getcwd()\n",
    "# change to model dir\n",
    "os.chdir(str(model_tar_dir))\n",
    "# use pigz for faster and parallel compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2fbdff-8f3e-44db-a8ac-8fec3345ee9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -cf model.tar.gz --use-compress-program=pigz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063fff7-63fc-40ed-974c-cc3978237fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change back to parent dir\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b69de-66c4-48f7-978f-b49bf0125c4d",
   "metadata": {},
   "source": [
    "Upload the compressed model artifacts to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62740ff-025a-4748-b7d6-3a0ed0c97795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    " \n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri = S3Uploader.upload(local_path=str(model_tar_dir.joinpath(\"model.tar.gz\")), desired_s3_uri=f\"s3://{bucket}/{HF_MODEL_ID}\")\n",
    " \n",
    "print(f\"model uploaded to: {s3_model_uri}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c16eb-aae5-4c08-869d-779333a000e0",
   "metadata": {},
   "source": [
    "Now, you can create the Model Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14586391-e966-4b96-9728-0b4ab25ccf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sagemaker.utils.name_from_base(HF_MODEL_ID.replace(\"/\",\"-\"))\n",
    "\n",
    "config_s3 = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "}\n",
    "\n",
    "# create Hugging Face Model object\n",
    "model_bge_rerank = HuggingFaceModel(\n",
    "    image_uri=get_huggingface_llm_image_uri(\"huggingface-tei\"),\n",
    "    env=config_s3,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    model_data=s3_model_uri # S3 URI where the model artefacts are found\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b76a00-2293-4756-af2f-d5e923057cff",
   "metadata": {},
   "source": [
    "### Option 4: Create a DJL Model Object that uses SageMaker Large Model Inference (LMI) container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007e748-b75e-461f-9166-09642dbb08c4",
   "metadata": {},
   "source": [
    "DJL Serving is a high performance universal stand-alone model serving solution. It takes a deep learning model, several models, or workflows and makes them available through an HTTP endpoint.\n",
    "\n",
    "You can use one of the DJL Serving [Deep Learning Containers (DLCs)](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html) to serve your models on AWS. To learn about the supported model types and frameworks, see the [DJL Serving documentation](https://docs.djl.ai/master/index.html).\n",
    "\n",
    "In this notebook, we will use Large Model Inference (LMI) containers which are a set of high-performance Docker Containers purpose built for large language model (LLM) inference. With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b2082-e062-4ece-8102-7ebe0e6c44a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.djl_inference.model import DJLModel\n",
    "\n",
    "\n",
    "model_id = \"BAAI/bge-reranker-v2-m3\" # model will be download form Huggingface hub\n",
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\".format(region)\n",
    "\n",
    "\n",
    "env = {\n",
    "    \"SERVING_MIN_WORKERS\": \"1\",   # make sure min and max Workers are equals when deploy model on GPU\n",
    "    \"SERVING_MAX_WORKERS\": \"1\",\n",
    "    \"ARGS_RERANKING\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\"\n",
    "}\n",
    "\n",
    "# create DJL Model object\n",
    "model_bge_rerank = DJLModel(\n",
    "    model_id=model_id,\n",
    "    task=\"text-classification\",\n",
    "    image_uri=image_uri,\n",
    "    env=env,\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00865c9-711c-4fdf-a34c-1989e9aef7a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the Model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662baf5a-f2f7-4578-8ead-acb248f94bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bge_rerank_predictor = model_bge_rerank.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.xlarge\",\n",
    "    container_startup_health_check_timeout=300,\n",
    "    wait=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1aa68d-425b-44dd-b462-4deba1e683c1",
   "metadata": {},
   "source": [
    "Once the model is deployed, test the model invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacfe00-6080-410f-a5c4-bf31f88692f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isinstance(model_bge_rerank, DJLModel):\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"A panda is a type of bear that is known for its distinctive black and white coloring.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"Pandas are native to China and are known for their diet, which consists mostly of bamboo.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"Pandas have become a symbol of conservation due to their status as an endangered species.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"The panda's distinctive black and white coat serves as camouflage in its natural habitat.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"Pandas are known for their playful behavior and are a favorite among zoo visitors.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"There are two main species of panda: the giant panda and the red panda, which are not closely related.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"Pandas primarily live in temperate forests high in the mountains of southwest China.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"The giant panda has a large head, heavy body, rounded ears, and a short tail.\"},\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"Efforts to protect panda habitats have led to the establishment of several panda reserves in China.\"}\n",
    "        ]\n",
    "    }\n",
    "else:\n",
    "    payload = {\n",
    "    \"query\": \"what is panda?\",\n",
    "    \"texts\": [\n",
    "            \"A panda is a type of bear that is known for its distinctive black and white coloring.\",\n",
    "            \"Pandas are native to China and are known for their diet, which consists mostly of bamboo.\",\n",
    "            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "            \"Pandas have become a symbol of conservation due to their status as an endangered species.\",\n",
    "            \"The panda's distinctive black and white coat serves as camouflage in its natural habitat.\",\n",
    "            \"Pandas are known for their playful behavior and are a favorite among zoo visitors.\",\n",
    "            \"There are two main species of panda: the giant panda and the red panda, which are not closely related.\",\n",
    "            \"Pandas primarily live in temperate forests high in the mountains of southwest China.\",\n",
    "            \"The giant panda has a large head, heavy body, rounded ears, and a short tail.\",\n",
    "            \"Efforts to protect panda habitats have led to the establishment of several panda reserves in China.\"\n",
    "        ]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac76c8-c5eb-46c0-8343-df4ccc1b2564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bge_rerank_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db3f0d-4b86-49f8-aeea-5f270d15df4b",
   "metadata": {},
   "source": [
    "## Benchmark the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619a4df-27af-4f94-93df-39abfd48db2f",
   "metadata": {},
   "source": [
    "Create a benchmark scrip that sends concurrent requests, stores and plots the latencies and throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61353888-25e1-4549-be45-55b9500ac062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming predictor is already defined and initialized\n",
    "# and predictor.predict(data=payload) is the method to be benchmarked\n",
    "\n",
    "def benchmark_predictor(predictor, payload, steps, iterations=5):\n",
    "    \"\"\"\n",
    "    Benchmarks a predictor's performance by measuring latency and throughput \n",
    "    under varying levels of concurrent requests.\n",
    "    \n",
    "    Args:\n",
    "        predictor (object): The predictor object with a `predict` method.\n",
    "        payload (any): The input data to be sent to the predictor.\n",
    "        steps (list): A list of different numbers of concurrent requests to test.\n",
    "        iterations (int, optional): The number of iterations for each concurrency level. Default is 5.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Three lists containing the request counts, latencies, and throughputs.\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "    request_counts = []\n",
    "\n",
    "    def send_request():\n",
    "        \"\"\"Sends a single request to the predictor and measures its latency.\"\"\"\n",
    "        start_time = time.time()\n",
    "        resp = predictor.predict(data=payload)\n",
    "        latency = time.time() - start_time\n",
    "        return latency\n",
    "\n",
    "    for num_requests in steps:\n",
    "        iter_latencies = []\n",
    "        iter_throughputs = []\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Use ThreadPoolExecutor to send concurrent requests\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "                futures = [executor.submit(send_request) for _ in range(num_requests)]\n",
    "                latencies_batch = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate average latency for this iteration\n",
    "            latency = np.mean(latencies_batch)\n",
    "            # Calculate throughput for this iteration\n",
    "            throughput = num_requests / total_time\n",
    "            \n",
    "            iter_latencies.append(latency)\n",
    "            iter_throughputs.append(throughput)\n",
    "        \n",
    "        # Calculate average latency and throughput over all iterations\n",
    "        avg_latency = np.mean(iter_latencies)\n",
    "        avg_throughput = np.mean(iter_throughputs)\n",
    "        \n",
    "        latencies.append(avg_latency)\n",
    "        throughputs.append(avg_throughput)\n",
    "        request_counts.append(num_requests)\n",
    "        \n",
    "        # Print results for the current number of requests\n",
    "        print(f\"Requests: {num_requests}, Average Latency: {avg_latency:.4f}s, Average Throughput: {avg_throughput:.2f} req/s\")\n",
    "\n",
    "    return request_counts, latencies, throughputs\n",
    "\n",
    "def plot_metrics(request_counts, latencies, throughputs):\n",
    "    \"\"\"\n",
    "    Plots the benchmarking results, showing the average latency and throughput \n",
    "    as a function of the number of concurrent requests.\n",
    "    \n",
    "    Args:\n",
    "        request_counts (list): The list of different numbers of concurrent requests tested.\n",
    "        latencies (list): The list of average latencies corresponding to the request counts.\n",
    "        throughputs (list): The list of average throughputs corresponding to the request counts.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Number of Concurrent Requests')\n",
    "    ax1.set_ylabel('Average Latency (s)', color=color)\n",
    "    ax1.plot(request_counts, latencies, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:green'\n",
    "    ax2.set_ylabel('Throughput (requests/s)', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(request_counts, throughputs, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title('Latency and Throughput Benchmarking')\n",
    "    plt.show()\n",
    "\n",
    "def plot_latency_vs_throughput(latencies, throughputs, request_counts):\n",
    "    \"\"\"\n",
    "    Plots latency against throughput, with annotations for the number of concurrent requests.\n",
    "    \n",
    "    Args:\n",
    "        latencies (list): The list of average latencies.\n",
    "        throughputs (list): The list of average throughputs.\n",
    "        request_counts (list): The list of different numbers of concurrent requests tested.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(throughputs, latencies, 'o-')\n",
    "    plt.xlabel('Throughput (requests/s)')\n",
    "    plt.ylabel('Average Latency (s)')\n",
    "    plt.title('Latency vs Throughput')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Label each point with the request count\n",
    "    for i, request_count in enumerate(request_counts):\n",
    "        plt.annotate(request_count, (throughputs[i], latencies[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8365d-5cc5-4f9e-873a-bd435cf76dd6",
   "metadata": {},
   "source": [
    "Benchmark the endpoint and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54337c2-6361-4ea1-bde5-f7fe53140de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_benchmark = pd.DataFrame(columns=[\"client_batch_size\", \"concurrent_request_counts\", \"latencies\", \"throughputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3d5f2-698c-4c25-b8d0-df67add71e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_requests=0 # 2^0=1\n",
    "max_requests = 9 # 2^9=512\n",
    "step_size = 1\n",
    "iterations = 20\n",
    "client_batch_size = 32\n",
    "\n",
    "steps = list(map(lambda x:2**x,range(min_requests, max_requests, step_size)))\n",
    "\n",
    "if isinstance(model_bge_rerank, DJLModel):\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"text\": \"what is panda?\", \"text_pair\": \"A panda is a type of bear that is known for its distinctive black and white coloring.\"}\n",
    "        ] * client_batch_size\n",
    "    }\n",
    "else:\n",
    "    payload = {\n",
    "        \"query\": \"what is panda?\",\n",
    "        \"texts\": [\n",
    "            \"A panda is a type of bear that is known for its distinctive black and white coloring.\"\n",
    "        ] * client_batch_size\n",
    "    }\n",
    "\n",
    "\n",
    "request_counts, latencies, throughputs = benchmark_predictor(model_bge_rerank_predictor, payload, steps, iterations)\n",
    "plot_metrics(request_counts, latencies, throughputs)\n",
    "plot_latency_vs_throughput(latencies, throughputs, request_counts)\n",
    "\n",
    "new_data = {\n",
    "    'client_batch_size': [client_batch_size] * len(request_counts),\n",
    "    'concurrent_request_counts': request_counts,\n",
    "    'latencies': latencies,\n",
    "    'throughputs' : throughputs\n",
    "}\n",
    "\n",
    "df_benchmark = df_benchmark.append(pd.DataFrame(new_data), ignore_index=True)\n",
    "df_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e53c1-2a48-4a33-af64-6d42918e2653",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8e990-bcb2-49f0-935e-0768829a1b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bge_rerank_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
